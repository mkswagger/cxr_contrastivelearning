{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import numpy as np\n",
        "import time\n",
        "import copy\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.optim import lr_scheduler\n",
        "import cv2\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "nHIuY4g9qXQH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Counter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def compute_AUCs(gt, pred):\n",
        "    gt_np = gt.cpu().numpy()\n",
        "    pred_np = pred.cpu().numpy()\n",
        "    return roc_auc_score(gt_np[:,0], pred_np[:,0])\n",
        "\n",
        "def cross_auc(R_a_0, R_b_1):\n",
        "    scores = np.array(list(R_a_0.cpu().numpy()) + list(R_b_1.cpu().numpy()))\n",
        "    y_true = np.zeros(len(R_a_0)+len(R_b_1))\n",
        "    y_true[0:len(R_a_0)] = 1 # Pr[ LEFT > RIGHT]; Y = 1 is the left (A0)\n",
        "    return roc_auc_score(y_true, scores)\n",
        "\n"
      ],
      "metadata": {
        "id": "K3T3pjwOqXQL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def group_auc(labels, outputs, groups):\n",
        "    groups = {}\n",
        "    for i in range(4):\n",
        "        groups[\"group\"+str(i)+\"p\"]=[]\n",
        "        groups[\"group\"+str(i)+\"n\"]=[]\n",
        "\n",
        "    for i in range(len(labels)):\n",
        "        if groups[i] == 0:\n",
        "            if labels[i][0] == 1:\n",
        "                group0p.append(i)\n",
        "            if labels[i][0] == 0:\n",
        "                group0n.append(i)\n",
        "        if groups[i] == 1:\n",
        "            if labels[i][0] == 1:\n",
        "                group1p.append(i)\n",
        "            if labels[i][0] == 0:\n",
        "                group1n.append(i)\n",
        "        if groups[i] == 2:\n",
        "            if labels[i][0] == 1:\n",
        "                group2p.append(i)\n",
        "            if labels[i][0] == 0:\n",
        "                group2n.append(i)\n",
        "        if groups[i] == 3:\n",
        "            if labels[i][0] == 1:\n",
        "                group3p.append(i)\n",
        "            if labels[i][0] == 0:\n",
        "                group3n.append(i)\n",
        "\n",
        "    groupp = group0p+group1p+group2p+group3p\n",
        "    groupn = group0n+group1n+group2n+group3n\n",
        "    outputs_ = outputs.clone().detach().cpu()\n",
        "\n",
        "    try:\n",
        "        AUC = cross_auc(torch.index_select(outputs_,0,torch.tensor(groupp)), torch.index_select(outputs_,0,torch.tensor(groupn)))\n",
        "    except:\n",
        "        AUC = 1\n",
        "\n",
        "    Cross_auc = {}\n",
        "    total_squares = 4\n",
        "    for n in range(total_squares):\n",
        "        try:\n",
        "            Cross_auc['A' + str(n) + str(n)] = cross_auc(torch.index_select(outputs_,0,torch.tensor(groups[\"group\"+str(n)+\"p\"])), torch.index_select(outputs_,0,torch.tensor(\"group\"+str(n)+\"n\")))\n",
        "        except:\n",
        "            Cross_auc['A' + str(n) + str(n)] = 1\n",
        "        try:\n",
        "            Cross_auc['A' + str(n) + 'a'] = cross_auc(torch.index_select(outputs_,0,torch.tensor(groups[\"group\"+str(n)+\"p\"])), torch.index_select(outputs_,0,torch.tensor(\"group\"+str(n)+\"n\")))\n",
        "        except:\n",
        "            Cross_auc['A' + str(n) + 'a'] = 1\n",
        "        try:\n",
        "            Cross_auc['A' + 'a' + str(n)] = cross_auc(torch.index_select(outputs_,0,torch.tensor(groups[\"group\"+str(n)+\"p\"])), torch.index_select(outputs_,0,torch.tensor(\"group\"+str(n)+\"n\")))\n",
        "        except:\n",
        "            Cross_auc['A' + 'a' + str(n)] = 1\n",
        "\n",
        "    group_num = [len(group0p),len(group0n),len(group1p),len(group1n),len(group2p),len(group2n),len(group3p),len(group3n)]\n",
        "\n",
        "    return Cross_auc"
      ],
      "metadata": {
        "id": "2_Jx4yshqpn1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def criterion(outputs, labels, groups):\n",
        "\n",
        "    group0p = []\n",
        "    group0n = []\n",
        "    group1p = []\n",
        "    group1n = []\n",
        "    group2p = []\n",
        "    group2n = []\n",
        "    group3p = []\n",
        "    group3n = []\n",
        "\n",
        "    for i in range(len(labels)):\n",
        "        if groups[i] == 0:\n",
        "            if labels[i][0] == 1:\n",
        "                group0p.append(i)\n",
        "            if labels[i][0] == 0:\n",
        "                group0n.append(i)\n",
        "        if groups[i] == 1:\n",
        "            if labels[i][0] == 1:\n",
        "                group1p.append(i)\n",
        "            if labels[i][0] == 0:\n",
        "                group1n.append(i)\n",
        "        if groups[i] == 2:\n",
        "            if labels[i][0] == 1:\n",
        "                group2p.append(i)\n",
        "            if labels[i][0] == 0:\n",
        "                group2n.append(i)\n",
        "        if groups[i] == 3:\n",
        "            if labels[i][0] == 1:\n",
        "                group3p.append(i)\n",
        "            if labels[i][0] == 0:\n",
        "                group3n.append(i)\n",
        "\n",
        "    groupp = group0p+group1p+group2p+group3p\n",
        "    groupn = group0n+group1n+group2n+group3n\n",
        "    outputs_ = outputs.clone().detach().cpu()\n",
        "\n",
        "    try:\n",
        "        AUC = cross_auc(torch.index_select(outputs_,0,torch.tensor(groupp)), torch.index_select(outputs_,0,torch.tensor(groupn)))\n",
        "    except:\n",
        "        AUC = 1\n",
        "    try:\n",
        "        AUC0a = cross_auc(torch.index_select(outputs_,0,torch.tensor(group0p)), torch.index_select(outputs_,0,torch.tensor(groupn)))\n",
        "    except:\n",
        "        AUC0a = 1.1\n",
        "    try:\n",
        "        AUC1a = cross_auc(torch.index_select(outputs_,0,torch.tensor(group1p)), torch.index_select(outputs_,0,torch.tensor(groupn)))\n",
        "    except:\n",
        "        AUC1a = 1.1\n",
        "    try:\n",
        "        AUC2a = cross_auc(torch.index_select(outputs_,0,torch.tensor(group2p)), torch.index_select(outputs_,0,torch.tensor(groupn)))\n",
        "    except:\n",
        "        AUC2a = 1.1\n",
        "    try:\n",
        "        AUC3a = cross_auc(torch.index_select(outputs_,0,torch.tensor(group3p)), torch.index_select(outputs_,0,torch.tensor(groupn)))\n",
        "    except:\n",
        "        AUC3a = 1.1\n",
        "\n",
        "#     if AUC ==1 and AUC0a == 1 and AUC1a == 1:\n",
        "#         print('three auc are', AUC)\n",
        "    loss = nn.MarginRankingLoss(margin=0.05)\n",
        "\n",
        "    minimum = np.argsort(np.array([AUC0a, AUC1a, AUC2a, AUC3a, AUC]))[0]\n",
        "    # print(minimum)\n",
        "\n",
        "    if minimum == 0:\n",
        "        index0p = []\n",
        "        for i in group0p:\n",
        "            index0p.extend([i]*len(groupn))\n",
        "        index0an = (groupn)*len(group0p)\n",
        "        # print (index0p, index0an)\n",
        "        return loss(torch.index_select(outputs,0,torch.tensor(index0p).to(device)), torch.index_select(outputs,0,torch.tensor(index0an).to(device)), (torch.ones(len(index0p),1)*1).to(device))\n",
        "\n",
        "    elif minimum == 1:\n",
        "        index1p = []\n",
        "        for i in group1p:\n",
        "            index1p.extend([i]*len(groupn))\n",
        "        index1an = (groupn)*len(group1p)\n",
        "\n",
        "        return loss(torch.index_select(outputs,0,torch.tensor(index1p).to(device)), torch.index_select(outputs,0,torch.tensor(index1an).to(device)), (torch.ones(len(index1p),1)*1).to(device))\n",
        "\n",
        "\n",
        "    elif minimum == 2:\n",
        "        index2p = []\n",
        "        for i in group2p:\n",
        "            index2p.extend([i]*len(groupn))\n",
        "        index2an = (groupn)*len(group2p)\n",
        "\n",
        "        return loss(torch.index_select(outputs,0,torch.tensor(index2p).to(device)), torch.index_select(outputs,0,torch.tensor(index2an).to(device)), (torch.ones(len(index2p),1)*2).to(device))\n",
        "\n",
        "    elif minimum == 3:\n",
        "        index3p = []\n",
        "        for i in group3p:\n",
        "            index3p.extend([i]*len(groupn))\n",
        "        index3an = (groupn)*len(group3p)\n",
        "\n",
        "        return loss(torch.index_select(outputs,0,torch.tensor(index3p).to(device)), torch.index_select(outputs,0,torch.tensor(index3an).to(device)), (torch.ones(len(index3p),1)*1).to(device))\n",
        "\n",
        "    else:\n",
        "        indexp = []\n",
        "        for i in groupp:\n",
        "            indexp.extend([i]*len(groupn))\n",
        "        indexn = (groupn)*len(groupp)\n",
        "\n",
        "        return loss(torch.index_select(outputs,0,torch.tensor(indexp).to(device)), torch.index_select(outputs,0,torch.tensor(indexn).to(device)), (torch.ones(len(indexp),1)*1).to(device))\n"
      ],
      "metadata": {
        "id": "4D80svcLqx4R"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_model(dataloaders,model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "def train_model(dataloaders, filename, model, criterion, optimizer, num_epochs=25):\n",
        "    since = time.time()\n",
        "    fopen = open(filename, \"w\")\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_AUROC_avg = 0.0\n",
        "    losses = Counter()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 100)\n",
        "\n",
        "\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            gt = torch.FloatTensor().to(device)\n",
        "            pred = torch.FloatTensor().to(device)\n",
        "            losses.reset()\n",
        "            groups = []\n",
        "\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            # Iterate over data.\n",
        "            t = tqdm(enumerate(dataloaders[phase]),  desc='Loss: **** ', total=len(dataloaders[phase]), bar_format='{desc}{bar}{r_bar}')\n",
        "            for batch_idx, (inputs, labels, group) in t:\n",
        "                # if batch_idx == 0:\n",
        "                #     continue\n",
        "                # print(torch.isnan(inputs).sum())\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                #print(inputs.shape, labels.shape)\n",
        "                # print('the lables is',torch.unique(labels))\n",
        "                if len(torch.unique(labels)) !=1 and len(np.unique(group) != 1):\n",
        "                    # print(len(torch.unique(labels)))\n",
        "                    # zero the parameter gradients\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # forward\n",
        "                    # track history if only in train\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        outputs = model(inputs)\n",
        "\n",
        "                        gt = torch.cat((gt, labels), 0)\n",
        "                        pred = torch.cat((pred, outputs.data), 0)\n",
        "                        groups += group\n",
        "\n",
        "                        # print('outputs shape',outputs.shape)\n",
        "                        # print('labels shape', labels.shape)\n",
        "                        # print('groups shape', group.shape)\n",
        "                        loss = criterion(outputs, labels, group)\n",
        "\n",
        "                        # backward + optimize only if in training phase\n",
        "                        if phase == 'train':\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                    # statistics\n",
        "                    losses.update(loss.data.item(), inputs.size(0))\n",
        "                    t.set_description('Loss: %.3f ' % (losses.avg))\n",
        "\n",
        "            AUCs = compute_AUCs(gt, pred)\n",
        "            AUROC_avg = AUCs\n",
        "            AUC, A00, A11, A22, A33, A0a, A1a, A2a, A3a, Aa0, Aa1, Aa2, Aa3, group_num = group_auc(gt, pred, groups)\n",
        "\n",
        "            if phase == \"val\":\n",
        "\n",
        "                # scheduler.step(losses.avg)\n",
        "\n",
        "                if best_AUROC_avg < AUROC_avg:\n",
        "                    best_AUROC_avg = AUROC_avg\n",
        "                    torch.save(model.state_dict(), \"/prj0129/mil4012/glaucoma/NIH-chest-x-ray/CXR8/weights/densenet201_mimic_gender_age_nofinding4.pth\")\n",
        "                fopen.write('\\nEpoch {} \\t [{}] : \\t {AUROC_avg:.3f}\\n'.format(epoch, phase, AUROC_avg=AUROC_avg))\n",
        "                fopen.write('{} \\t {}\\n'.format(CLASS_NAMES, AUCs))\n",
        "                fopen.write('-' * 100)\n",
        "\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})'.format(\n",
        "                   epoch, batch_idx + 1, len(dataloaders[phase]), loss=losses))\n",
        "            print('{} : \\t {AUROC_avg:.3f}'.format(phase, AUROC_avg=AUROC_avg))\n",
        "            print('AUC',AUC)\n",
        "            print('A00',A00)\n",
        "            print('A11',A11)\n",
        "            print('A22',A22)\n",
        "            print('A33',A33)\n",
        "            print('A0a',A0a)\n",
        "            print('A1a',A1a)\n",
        "            print('A2a',A2a)\n",
        "            print('A3a',A3a)\n",
        "            print('Aa0',Aa0)\n",
        "            print('Aa1',Aa1)\n",
        "            print('Aa2',Aa2)\n",
        "            print('Aa3',Aa3)\n",
        "            print('Group Num',group_num)\n",
        "\n",
        "            fopen.flush()\n",
        "    fopen.close()\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "vG2YkHudq2cV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(test_loader,model):\n",
        "    model.eval()\n",
        "    gt = torch.FloatTensor().to(device)\n",
        "    pred = torch.FloatTensor().to(device)\n",
        "    groups = []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, labels, group) in enumerate(test_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            gt = torch.cat((gt, labels), 0)\n",
        "            pred = torch.cat((pred, outputs.data), 0)\n",
        "            groups += group\n",
        "    AUCs = compute_AUCs(gt, pred)\n",
        "    AUC, A00, A11, A22, A33, A0a, A1a, A2a, A3a, Aa0, Aa1, Aa2, Aa3, group_num = group_auc(gt, pred, groups)\n",
        "    print('AUCs',AUCs)\n",
        "    print('AUC',AUC)\n",
        "    print('A00',A00)\n",
        "    print('A11',A11)\n",
        "    print('A22',A22)\n",
        "    print('A33',A33)\n",
        "    print('A0a',A0a)\n",
        "    print('A1a',A1a)\n",
        "    print('A2a',A2a)\n",
        "    print('A3a',A3a)\n",
        "    print('Aa0',Aa0)\n",
        "    print('Aa1',Aa1)\n",
        "    print('Aa2',Aa2)\n",
        "    print('Aa3',Aa3)\n",
        "    print('Group Num',group_num)\n",
        "    pred1 = pred.cpu()\n",
        "    pred2 = pred1.numpy()\n",
        "    gt1 = gt.cpu()\n",
        "    gt2 = gt1.numpy()\n",
        "#     groups1 = groups.cpu()\n",
        "#     groups2 = groups1.numpy\n",
        "    np.savez('/prj0129/mil4012/glaucoma/NIH-chest-x-ray/CXR8/Result/densenet201_mimic_gender_age_nofinding4.npz', prediction=pred2, label=gt2, group=groups)\n",
        "    np.savetxt('/prj0129/mil4012/glaucoma/NIH-chest-x-ray/CXR8/Result/densenet201_mimic_gender_age_nofinding4.txt', pred2)\n"
      ],
      "metadata": {
        "id": "qFe7I1Skq6vl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path_train = \"/path/to/training/image/data.csv\"\n",
        "\n",
        "#training dataset\n",
        "tmp = np.loadtxt(image_path_train, dtype=np.str, delimiter=\",\")\n",
        "train_path = tmp[:,0]\n",
        "train_path = train_path[1:len(train_path)]"
      ],
      "metadata": {
        "id": "NSkg8UL0sNKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    train_sampler = None\n",
        "    batch_size = 96\n",
        "    workers = 4\n",
        "    N_CLASSES = 1\n",
        "    CLASS_NAMES = 'MIMIC'\n",
        "\n",
        "\n",
        "    #get data and label for training, validate, and testing dataset.\n",
        "\n",
        "    #training dataset\n",
        "    image_path_train = \"C:/Users/Chitra/Downloads/COVID-19_Radiography_Dataset\"\n",
        "\n",
        "#training dataset\n",
        "\n",
        "    tmp = np.loadtxt(image_path_train, dtype=np.str, delimiter=\",\")\n",
        "    train_path = tmp[:,0]\n",
        "    train_path = train_path[1:len(train_path)]\n",
        "    # train_path = train_path[1:2000]\n",
        "    #7-> lung lesion, 12->pneumonia, 13->pneumothorax, 9->no finding, 16->gender, 17->age, 18->race\n",
        "    labels = tmp[:,9]\n",
        "    print('the disease is',labels[0])\n",
        "    labels = labels[1:len(labels)]\n",
        "    # labels = labels[1:2000]\n",
        "    gender = tmp[:,16]\n",
        "    gender = gender[1:len(gender)]\n",
        "\n",
        "    age = tmp[:,17]\n",
        "    age = age[1:len(age)]\n",
        "\n",
        "    race = tmp[:,18]\n",
        "    race = race[1:len(race)]\n",
        "\n",
        "\n",
        "    train_label = copy.deepcopy(labels)\n",
        "    ind = np.argwhere(labels=='1.0')\n",
        "    train_label[ind] = 1\n",
        "    ind = np.argwhere(labels!='1.0')\n",
        "    train_label[ind] = 0\n",
        "    train_label = np.asarray(train_label, dtype=int)\n",
        "\n",
        "    ##gender and age\n",
        "    train_groups = copy.deepcopy(gender)\n",
        "    age = np.asarray(age, dtype=float)\n",
        "    ind = np.argwhere(gender=='M')\n",
        "    ind1 = np.argwhere(age < 60)\n",
        "    ind2 = np.intersect1d(ind, ind1)\n",
        "    train_groups[ind2] = 0\n",
        "\n",
        "    ind = np.argwhere(gender=='M')\n",
        "    ind1 = np.argwhere(age >= 60)\n",
        "    ind2 = np.intersect1d(ind, ind1)\n",
        "    train_groups[ind2] = 1\n",
        "\n",
        "    ind = np.argwhere(gender=='F')\n",
        "    ind1 = np.argwhere(age < 60)\n",
        "    ind2 = np.intersect1d(ind, ind1)\n",
        "    train_groups[ind2] = 2\n",
        "\n",
        "    ind = np.argwhere(gender=='F')\n",
        "    ind1 = np.argwhere(age >= 60)\n",
        "    ind2 = np.intersect1d(ind, ind1)\n",
        "    train_groups[ind2] = 3\n",
        "    train_groups = np.asarray(train_groups, dtype=int)\n",
        "\n",
        "\n",
        "    #val dataset\n",
        "    tmp = np.loadtxt(image_path_val  , dtype=np.str, delimiter=\",\")\n",
        "    val_path = tmp[:,0]\n",
        "    val_path = val_path[1:len(val_path)]\n",
        "    #7-> lung lesion, 12->pneumonia, 13->pneumothorax, 9->no finding, 16->gender, 17->age, 18->race\n",
        "    labels = tmp[:,9]\n",
        "    print('the disease is',labels[0])\n",
        "    labels = labels[1:len(labels)]\n",
        "\n",
        "    gender = tmp[:,16]\n",
        "    gender = gender[1:len(gender)]\n",
        "\n",
        "    age = tmp[:,17]\n",
        "    age = age[1:len(age)]\n",
        "\n",
        "    race = tmp[:,18]\n",
        "    race = race[1:len(race)]\n",
        "\n",
        "\n",
        "    val_label = copy.deepcopy(labels)\n",
        "    ind = np.argwhere(labels=='1.0')\n",
        "    val_label[ind] = 1\n",
        "    ind = np.argwhere(labels!='1.0')\n",
        "    val_label[ind] = 0\n",
        "    val_label = np.asarray(val_label, dtype=int)\n",
        "\n",
        "\n",
        "    ##gender and age\n",
        "    val_groups = copy.deepcopy(gender)\n",
        "    age = np.asarray(age, dtype=float)\n",
        "    ind = np.argwhere(gender=='M')\n",
        "    ind1 = np.argwhere(age < 60)\n",
        "    ind2 = np.intersect1d(ind, ind1)\n",
        "    val_groups[ind2] = 0\n",
        "\n",
        "    ind = np.argwhere(gender=='M')\n",
        "    ind1 = np.argwhere(age >= 60)\n",
        "    ind2 = np.intersect1d(ind, ind1)\n",
        "    val_groups[ind2] = 1\n",
        "\n",
        "    ind = np.argwhere(gender=='F')\n",
        "    ind1 = np.argwhere(age < 60)\n",
        "    ind2 = np.intersect1d(ind, ind1)\n",
        "    val_groups[ind2] = 2\n",
        "\n",
        "    ind = np.argwhere(gender=='F')\n",
        "    ind1 = np.argwhere(age >= 60)\n",
        "    ind2 = np.intersect1d(ind, ind1)\n",
        "    val_groups[ind2] = 3\n",
        "\n",
        "\n",
        "    val_groups = np.asarray(val_groups, dtype=int)\n",
        "\n",
        "    #test dataset\n",
        "    tmp = np.loadtxt(image_path_train, dtype=np.str, delimiter=\",\")\n",
        "    test_path = tmp[:,0]\n",
        "    test_path = test_path[1:len(test_path)]\n",
        "    #7-> lung lesion, 12->pneumonia, 13->pneumothorax, 9->no finding, 16->gender, 17->age, 18->race\n",
        "    labels = tmp[:,9]\n",
        "    print('the disease is',labels[0])\n",
        "    labels = labels[1:len(labels)]\n",
        "\n",
        "    gender = tmp[:,16]\n",
        "    gender = gender[1:len(gender)]\n",
        "\n",
        "    age = tmp[:,17]\n",
        "    age = age[1:len(age)]\n",
        "\n",
        "    race = tmp[:,18]\n",
        "    race = race[1:len(race)]\n",
        "\n",
        "\n",
        "    test_label = copy.deepcopy(labels)\n",
        "    ind = np.argwhere(labels=='1.0')\n",
        "    test_label[ind] = 1\n",
        "    ind = np.argwhere(labels!='1.0')\n",
        "    test_label[ind] = 0\n",
        "    test_label = np.asarray(test_label, dtype=int)\n",
        "\n",
        "\n",
        "    ##gender and age\n",
        "    test_groups = copy.deepcopy(gender)\n",
        "    age = np.asarray(age, dtype=float)\n",
        "    ind = np.argwhere(gender=='M')\n",
        "    ind1 = np.argwhere(age < 60)\n",
        "    ind2 = np.intersect1d(ind, ind1)\n",
        "    test_groups[ind2] = 0\n",
        "\n",
        "    ind = np.argwhere(gender=='M')\n",
        "    ind1 = np.argwhere(age >= 60)\n",
        "    ind2 = np.intersect1d(ind, ind1)\n",
        "    test_groups[ind2] = 1\n",
        "\n",
        "    ind = np.argwhere(gender=='F')\n",
        "    ind1 = np.argwhere(age < 60)\n",
        "    ind2 = np.intersect1d(ind, ind1)\n",
        "    test_groups[ind2] = 2\n",
        "\n",
        "    ind = np.argwhere(gender=='F')\n",
        "    ind1 = np.argwhere(age >= 60)\n",
        "    ind2 = np.intersect1d(ind, ind1)\n",
        "    test_groups[ind2] = 3\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    test_groups = np.asarray(test_groups, dtype=int)\n",
        "\n",
        "    train_label = train_label.astype(np.float)\n",
        "    val_label = val_label.astype(np.float)\n",
        "    test_label = test_label.astype(np.float)\n",
        "\n",
        "    train_label = np.reshape(train_label,(len(train_label),1))\n",
        "    val_label = np.reshape(val_label,(len(val_label),1))\n",
        "    test_label = np.reshape(test_label,(len(test_label),1))\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "VML34vF3q-c1",
        "outputId": "88dd6b00-2796-46e1-fd70-e24ea12bfb3b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-1978efd9fd58>:17: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  tmp = np.loadtxt(image_path_train, dtype=np.str, delimiter=\",\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "C:/Users/Chitra/Downloads/COVID-19_Radiography_Dataset not found.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1978efd9fd58>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtrain_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtrain_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[1;32m   1336\u001b[0m         \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1338\u001b[0;31m     arr = _read(fname, dtype=dtype, comment=comment, delimiter=delimiter,\n\u001b[0m\u001b[1;32m   1339\u001b[0m                 \u001b[0mconverters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiplines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskiprows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m                 \u001b[0munpack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    531\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    532\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{path} not found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: C:/Users/Chitra/Downloads/COVID-19_Radiography_Dataset not found."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomRotation(10),\n",
        "            # transforms.ToPILImage(),\n",
        "            transforms.Resize(224),\n",
        "            # transforms.CenterCrop(224),\n",
        "            transforms.RandomHorizontalFlip(0.5),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            # transforms.CenterCrop(224),\n",
        "            # transforms.ToPILImage(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    train_dataset = Dataset(train_path,train_label,groups=train_groups,transform = data_transforms[\"train\"])\n",
        "    val_dataset = Dataset(val_path,val_label,groups=val_groups,transform = data_transforms[\"val\"])\n",
        "    test_dataset = Dataset(test_path,test_label,groups=test_groups,transform = data_transforms[\"val\"])\n",
        "\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=(train_sampler is None),\n",
        "                                           num_workers=workers, pin_memory=True, sampler=train_sampler)\n",
        "\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=(train_sampler is None),\n",
        "                                           num_workers=workers, pin_memory=True, sampler=train_sampler)\n",
        "\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                                           num_workers=workers, pin_memory=True, sampler=train_sampler)\n",
        "\n",
        "    dataloaders = {\"train\": train_loader, \"val\": val_loader}\n",
        "\n",
        "    model_ft = models.densenet201(pretrained=True)\n",
        "    num_ftrs = model_ft.classifier.in_features\n",
        "    model_ft.classifier = nn.Sequential(\n",
        "                nn.Linear(num_ftrs, N_CLASSES),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.00005)\n",
        "\n",
        "    # optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    # Decay LR by a factor of 0.1 every 7 epochs\n",
        "#     exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, 'min', patience=2, eps=1e-08, verbose=True)\n",
        "\n",
        "    # model_ft = train_model(dataloaders, model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "    #                        num_epochs=20)\n",
        "    model_ft = train_model(dataloaders, model_ft, criterion, optimizer_ft,num_epochs=20)\n",
        "    model_ft.load_state_dict(torch.load(\"/prj0129/mil4012/glaucoma/NIH-chest-x-ray/CXR8/weights/densenet201_mimic_gender_age_nofinding4.pth\"))\n",
        "    test_model(test_loader,model_ft)"
      ],
      "metadata": {
        "id": "wwsFkRYyrDK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D2z_DYasv6tZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}